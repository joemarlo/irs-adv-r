---
title: "IRS Advanced R Live Code"
author: "Joe Marlo"
date: 8/17/2023
format: html
---

# R as a calculator

## Basic math

```{r}
1 + 1
2 * 2
5 / 2

5 / (1+2)

# PEMDAS
# paren, exp, multiplication, division, addition, subtraction

2^10
2**10

sqrt(10)

```

## Vectorized operations

```{r}
1:10 * 2
1:10 + 2

1:10 + 1:10

1:10 * 1:2
```

## Logicals

```{r}
1 == 1
a = 1

1 != 1
2 != 1

1:4 == 3

!TRUE

4 > 3

!(4 > 3)

my_vec <- 1:4 == 3
length(my_vec)

any(my_vec)
all(my_vec)
!all(my_vec)
```

```{r}
ifelse(1 == 1, 'this is true', 'this is false')
```

## Objects

```{r}
class(1)
1L
TRUE

'my string'
"my string"

"this is my 'quote'"
class('aslkdj')

list(1, 2, 3)
list(slot1 = 1, my_2 = 2, third = 3)

my_list <- list(slot1 = 1, my_2 = 2, third = 3)

my_list$slot1
```

## Indexing

```{r}
x <- letters

# indexing based on position
x[1]
x[2]

x[1:3]
x[10:15]
x[length(x)]
x[c(1:3, 5:6)]

# indexing based on logical
x <- x[1:6]

x[c(TRUE, TRUE, FALSE, FALSE, TRUE, TRUE)]
# 'a' 'b' 'c' 'd' 'e' 'f'

which(c(TRUE, TRUE, FALSE, FALSE, TRUE, TRUE))

x == 'b'
which(x == 'b')
```

## Data.frames

```{r}
mtcars

dim(mtcars)
ncol(mtcars)
nrow(mtcars)
colnames(mtcars)

summary(mtcars)
str(mtcars)

head(mtcars)

mtcars$mpg
mtcars[['mpg']]
mtcars['mpg']

mtcars[1,]
mtcars[1, 1]

mtcars[1:10,]
mtcars[c(1, 3, 5),]
mtcars[, 1:3]


mtcars[mtcars$mpg > 20, ]

mtcars$mpg > 20 & mtcars$hp > 100
mtcars[mtcars$mpg > 20 & mtcars$hp > 100, ]
```

## Help menu

```{r}
?mtcars
?nrow
?str
```

## Reading in data

```{r}
# connecting to a database
# con <- DBI::dbConnect(
#   odbc::odbc(),
#   database = '',
#   user = '',
#   password = ''
# )

# DBI::dbListTables(con)
```

```{r}
# getwd()
# here::here()
read.csv('../data/mtcars.csv', stringsAsFactors = FALSE)
mtcars <- readr::read_csv('../data/mtcars.csv')
```

## Basic plotting

```{r}
plot(
  x = mtcars$disp,
  y = mtcars$mpg
)

plot(
  x = mtcars$disp,
  y = mtcars$mpg,
  type = 'l'
)

barplot(mtcars$disp, names.arg = mtcars$car)

plot(
  x = mtcars$disp,
  y = mtcars$mpg,
  col = mtcars$cyl,
  main = 'My special plot',
  xlab = 'Displacement',
  ylab = 'Miles per gallon'
)
legend(
  'topright',
  c('4', '6', '8'),
  pch = 1,
  col = c('blue', 'violet', 'grey')
)

my_mod <- lm(mpg ~ disp, data = mtcars)

plot(
  x = mtcars$disp,
  y = mtcars$mpg,
  col = mtcars$cyl,
  main = 'My special plot',
  xlab = 'Displacement',
  ylab = 'Miles per gallon'
)
lines(
  x = mtcars$disp,
  y = predict(my_mod, newdata = list(disp = mtcars$disp))
)

hist(mtcars$disp)
plot(density(mtcars$disp))
```

## ggplot2

```{r}
library(ggplot2)

ggplot(
  mtcars,
  aes(x = disp, y = mpg, color = as.factor(cyl))
) +
  geom_point() +
  # geom_histogram() +
  # geom_line()
  facet_wrap(~am) + 
  # scale_color_viridis_d() +
  labs(title = 'My great plot',
       subtitle = 'My subtitle',
       x = 'Displacement',
       y = 'MPG',
       color = 'n cylinders')

ggplot(
  mtcars,
  aes(x = disp)
) +
  geom_histogram(bins = 5) +
  theme_bw()
```

## Functions

```{r}
?ggplot

square <- function(x) x^2
square <- function(x) {
  square_x <- x^2
  return(square_x)
}
x <- 10
square(2)
square(10)

apply()
sapply()
vapply()

sapply(1:10, 2:11, square)

square_and_add <- function(x, y){
  (x^2) + (y^2)
}

purrr::map2(1:10, 2:11, square_and_add)
purrr::map2_dbl(1:10, 2:11, square_and_add)
purrr::map2

for (i in 1:10){
  print(i * 2)
}
```

## Lambda functions

```{r}
square <- function(x) x^2
sapply(1:10, square)
sapply(1:10, function(x) x^2)
sapply(1:10, \(x) x^2)
```

## Generating data

```{r}
rnorm(100, mean = 0, sd = 1)
plot(density(rnorm(100000, mean = 0, sd = 1)))
```

```{r}
rbinom(n = 100, size = 1, prob = 0.3)
table(rbinom(n = 100, size = 1, prob = 0.3))
```

```{r}
set.seed(44)
my_binom <- rbinom(n = 100, size = 1, prob = 0.3)
table(my_binom)

barplot(table(my_binom))
```

```{r}
?Distributions
```

## Descriptive statistics

```{r}
mean()
max()
min()
median()
quantile()
weighted.mean()
var()
sd()

summary()
```

```{r}
my_normal <- rnorm(500, mean = 10, sd = 3)
plot(density(my_normal))

mean(my_normal)
max(my_normal)
min(my_normal)
range(my_normal)
quantile(my_normal, probs = c(0.1, 0.5, 0.9))

summary(my_normal)
```

```{r}
grades <- c(95, 72, 87, NA)
weights <- c(1/2, 1/3, 1/4, NA)
mean(grades)
weighted.mean(x = grades, w = weights, na.rm = TRUE)
```

```{r}
mean(my_normal)
my_normal[10] <- NA
mean(my_normal, na.rm = TRUE)
```

$$
  Var(x) = \frac{\sum_{i=1}^N (x_i - \bar{x})^2}{N - 1}
$$

```{r}
x <- rnorm(100, mean = 10, sd = 5)
plot(density(x))

var(x)
sd(x)
```

```{r}
cor(
  rnorm(100),
  rnorm(100)
)

x <- rnorm(100)
y <- x^2 + rnorm(100, sd = 0.5)
plot(x, y)
cor(x, y)
```

```{r}
cor(mtcars)
```

```{r}
my_pois <- rpois(100, lambda = 10)

tab <- table(my_pois)
tab_names <- names(tab)
max_val <- which.max(tab)
names(max_val)

mode <- function(x){
  tab <- table(x)
  tab_names <- names(tab)
  max_val <- which.max(tab)
  names(max_val)
}
mode(my_pois)

# sapply()
```

# tests

## t tests

```{r}
set.seed(44)

samp_1 <- rnorm(30, mean = 85, sd = 3)
samp_2 <- rnorm(30, mean = 75, sd = 3)
mean(samp_1)
mean(samp_2)

plot(density(samp_1))
lines(density(samp_2))

t.test(samp_1, samp_2)
```
## chi square

```{r}
data_2010 <- data.frame(
  type = c('Nissan', 'Mazda', 'Toyota', 'Honda'),
  perc = c(0.18, 0.1, 0.35, 0.37)
)

data_2011 <- data.frame(
  type = c('Nissan', 'Mazda', 'Toyota', 'Honda'),
  perc = c(0.15, 0.065, 0.385, 0.4)
)
barplot(data_2010$perc)
barplot(data_2011$perc)

chisq.test(
  data_2011$perc * 1000,
  p = data_2010$perc
)
```
## Tests for normality

```{r}
samp_1 <- rnorm(100)

qqnorm(samp_1)
qqline(samp_1)

# shapiro wilks
shapiro.test(samp_1)
shapiro.test(runif(1000))
```

# Regression


```{r}
mtcars
plot(mtcars$hp, mtcars$mpg)

my_mod <- lm(mpg ~ hp, data = mtcars)
# lm(mtcars$mpg ~ mtcars$hp)
my_mod
summary(my_mod)

plot(mtcars$hp, mtcars$mpg)
lines(x = mtcars$hp, my_mod$fitted.values)
```

```{r}
predict(my_mod)
predict(my_mod, list(hp = 1))
predict(my_mod, list(hp = 1000))
```


## The formula interface

```{r}
formula(mpg ~ hp)
formula(mpg ~ hp + disp)
formula(mpg ~ hp * disp)
# mpg ~ hp + disp + hp:disp
# formula(mpg ~ hp + I(disp))
```


## Multiple regression

```{r}
my_formula <- formula(mpg ~ hp + disp + wt)
plot(my_formula, data = mtcars)

my_m_mod <- lm(my_formula, data = mtcars)
summary(my_m_mod)

broom::tidy(my_m_mod)
# coef(my_m_mod)
```

## Visualizing models

```{r}
plot(my_formula, data = mtcars)

plot(mtcars$hp, mtcars$mpg)
lines(x = mtcars$hp, my_mod$fitted.values, col = 'red')

coefplot::coefplot(my_m_mod, intercept = FALSE)

plot(my_m_mod)
my_m_mod |> plot()

# library(magrittr)
# %>% 
# |>  

library(ggplot2)

my_m_mod |> 
  broom::tidy() |> 
  dplyr::filter(term != '(Intercept)', term != 'wt') |> 
  dplyr::mutate(
    lower = estimate - (qnorm(1-0.05) * std.error),
    upper = estimate + (qnorm(1-0.05) * std.error),
    is_sig = p.value <= 0.05
  ) |> 
  ggplot(aes(x = estimate, y = term, color = is_sig)) +
  geom_point() +
  geom_linerange(aes(xmin = lower, xmax = upper)) +
  geom_vline(xintercept = 0)

ggplot(
  mtcars,
  aes(y = mpg, x = hp, color = as.factor(vs))
) +
  geom_point() +
  facet_wrap(~am) +
  geom_smooth(method = 'lm')
```

## Logistic regression

```{r}
my_lin_mod <- lm(am ~ mpg, data = mtcars)
broom::tidy(my_lin_mod)
predict(my_lin_mod, list(mpg = 50))
predict(my_lin_mod, list(mpg = 0:50)) |> plot(type = 'l')
```

```{r}
my_log_mod <- glm(am ~ mpg, data = mtcars, family = 'binomial')
broom::tidy(my_log_mod)

predict(my_log_mod, list(mpg = 50), type = 'response')
predict(my_log_mod, list(mpg = 0:50), type = 'response') |> plot(x = 0:50, type = 'l')
```

```{r}
broom::tidy(my_log_mod)

odds <- exp(0.3070282)
odds / (1 + odds)
```

```{r}
predict(my_log_mod, list(mpg = 50), type = 'response')
```


## Count data

```{r}
rpois(1000, lambda = 10) |> hist()
rpois(10, lambda = 10)
```

```{r}
glm(cyl ~ ., family = 'poisson', data = mtcars) |> summary()
```

```{r}
hist(mtcars$cyl)
mean(mtcars$cyl)
var(mtcars$cyl)

my_nb_mod <- MASS::glm.nb(cyl ~ ., data = mtcars)
summary(my_nb_mod)
predict(my_nb_mod, type = 'response')
```

## zip model

```{r}
counts <- rbinom(32, size = 1, prob = 0.7) * rpois(32, 5)
plot(1:32, counts, type = 'l')
barplot(table(counts))

pscl::zeroinfl(counts ~ mtcars$cyl) |> summary()
```

# Model evaluation 

```{r}
plot(mtcars$hp, mtcars$mpg)
my_mod <- lm(mpg ~ hp, data = mtcars)
y_hat <- predict(my_mod, newdata = list(hp = mtcars$hp))

plot(mtcars$hp, mtcars$mpg)
lines(mtcars$hp, y_hat)

plot(y_hat - mtcars$mpg)
abline(0, 0, lty = 'dashed')
```

RMSE: root-mean-squared-error

```{r}
y <- mtcars$mpg
sqrt(mean((y_hat - y)^2))
```


MAE: mean absolute error

```{r}
mean(abs(y - yhat))
```

MAPE: mean absolute percentage error


AIC: Akaike's Information Criteria
AIC = -2 * log(L) + 2 * k
  L = max liklihood
  k = number of parameters

BIC: Bayesian Information Criteria
  - similar to AIC but punishes the addition of parameters

Trying to minimize these values

```{r}
mod_simple <- lm(mpg ~ hp, data = mtcars)
AIC(mod_simple)
BIC(mod_simple)
broom::glance(mod_simple)

mod_complex <- lm(mpg ~ ., data = mtcars)
broom::glance(mod_complex)
```









